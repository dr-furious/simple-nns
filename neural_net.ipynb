{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks - Assignment 3 for the Artificial Intelligence Course\n",
    "- author: **Martin SivÃ¡k**\n",
    "- AIS ID: **116291**\n",
    "- Date: **09.12.2023**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment Description\n",
    "In this assignment we were instructed to design a neural network, which would be able to classify data from a known dataset (from the *scikit-learn* module). We had to design three neural networks with different architectures and divide the input dataset into a train and test subset. Next we had to train a neural networks and evaluate its performance with different metrics and graphic visualizations.\n",
    "\n",
    "\n",
    "## Iris Dataset\n",
    "We chose the Iris dataset over other options (datasets) for its status as a well-known benchmark dataset with a manageable size of 150 samples. It facilitates multiclass classification tasks involving numeric features, has no missing values, and maintains balance among its classes (50 records per each class). Its simplicity, real-world relevance, and prevalence in tutorials make it an ideal choice for introducing and practicing machine learning concepts - which this assignment is.\n",
    "\n",
    "## Evaluation and Metrics\n",
    "Evaluation is essential for understanding how well the neural network model performs on an unseen data. It provides insights into the model's precision, recall, and overall accuracy. We evaluated the model on following key metrics:\n",
    "\n",
    "### Accuracy\n",
    "- **Definition**: The ratio of correctly predicted instances to the total instances.\n",
    "- **Example**: The model is correct 70% of the time.\n",
    "\n",
    "### Precision\n",
    "- **Definition**: The ability of the classifier not to label as positive a sample that is negative.\n",
    "- **Example**:\n",
    "- **Class 0 (1.00)**: All items predicted as Class 0 were indeed Class 0.\n",
    "- **Class 1 (0.00)**: None of the items predicted as Class 1 were actually Class 1.\n",
    "- **Class 2 (0.55)**: 55% of items predicted as Class 2 were actually Class 2.\n",
    "\n",
    "### Recall\n",
    "- **Definition**: The ability of the classifier to find all the positive instances.\n",
    "- **Example**:\n",
    "- **Class 0 (1.00)**: All actual Class 0 items were correctly predicted.\n",
    "- **Class 1 (0.00)**: None of the actual Class 1 items were predicted.\n",
    "- **Class 2 (55.00)**: 55% of the actual Class 2 items were correctly predicted.\n",
    "\n",
    "### F1-Score\n",
    "- **Definition**: The harmonic mean of precision and recall.\n",
    "- **Example**:\n",
    "- **Class 0 (1.00)**: The harmonic mean of precision and recall for Class 0.\n",
    "- **Class 1 (0.00)**: The harmonic mean of precision and recall for Class 1.\n",
    "- **Class 2 (0.71)**: The harmonic mean of precision and recall for Class 2.\n",
    "\n",
    "### Support\n",
    "- **Definition**: The number of actual instances of each class.\n",
    "- **Example**:\n",
    "- **Class 0 (10)**: Number of actual items in Class 0.\n",
    "- **Class 1 (9)**: Number of actual items in Class 1.\n",
    "- **Class 2 (11)**: Number of actual items in Class 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Inspecting the Iris Dataset\n",
    "\n",
    "Firstly, we load the famous Iris dataset from *scikit-learn* and store it in a Pandas DataFrame with tabular structure. The Iris dataset is a classic dataset in machine learning, containing measurements of **sepal length**, **sepal width**, **petal length**, and **petal width** for 150 iris flowers, with 50 samples from each of three different species.\n",
    "\n",
    "This initial code cell aims to load the dataset, provide basic information about its structure, and display the first few rows to give an overview of the data. We can see that our data looks very nice, all records posses only numeric values (floats or integers) and we have 0 missing values.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `load_iris`: A function from scikit-learn to load the Iris dataset.\n",
    "- `pd.DataFrame`: Creates a Pandas DataFrame to organize and manipulate the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data[\"target\"] = iris.target\n",
    "\n",
    "# Inspecting dataset\n",
    "print(data[\"target\"].value_counts())\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the Iris Dataset\n",
    "\n",
    "We can visualize the distributions of the measurements in the Iris dataset for each flower type (for each value in the `target` column). We utilize Matplotlib and Seaborn for visualizing the distributions of measurements in the Iris dataset by writing a small function that creates the visualizations and displays them in a single plot. From these visualizations we see that feature distributions are almost normally distributed, with a minimum number of outliers.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `matplotlib.pyplot`: Used for creating subplots and adjusting layout.\n",
    "- `seaborn`: Used for generating histograms with KDE (Kernel Density Estimation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_feature_dist(flower_type: int) -> None:\n",
    "    v_data = data[data[\"target\"] == flower_type]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, feature in enumerate(v_data.columns[:-1]):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        sns.histplot(v_data[feature], bins=20, kde=True)\n",
    "\n",
    "    # Adjust the vertical space between subplots\n",
    "    plt.subplots_adjust(wspace=0.75)\n",
    "\n",
    "visualize_feature_dist(0)\n",
    "visualize_feature_dist(1)\n",
    "visualize_feature_dist(2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the Dataset into Train and Test Sets\n",
    "\n",
    "Next we used the `train_test_split` function from scikit-learn to divide the Iris dataset into training and testing subsets. We decided to split the dataset using a 80-20 rule (80% of the dataset is used for training and 20% is used for testing). The 80-20 train-test split rule is a common choice, striking a balance between training set size for effective model learning and testing set size for robust performance evaluation. This split reduces overfitting concerns and ensures statistical significance in assessing the model's generalization to unseen data.\n",
    "\n",
    "The code outputs the following:\n",
    "- The shape of the training set (`X_train` and `y_train`).\n",
    "- The shape of the testing set (`X_test` and `y_test`).\n",
    "- The values of the training target variable (`y_train`).\n",
    "- The values of the testing target variable (`y_test`).\n",
    "\n",
    "The features (X) are the measurements of sepal length, sepal width, petal length, and petal width, while the target variable (y) is the species of the iris.\n",
    "\n",
    "This step is crucial for preparing the data for model training and evaluation. It ensures that there is a separate set of data for model training and a separate set of data (testing set) to evaluate the model's performance after training on the training set. It is important that the model is evaluated on an \"unseen\" data, otherwise the evaluation results would be biased and incorrect.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `train_test_split`: A function from scikit-learn to split the dataset into training and testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the dataset into a test and train subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(\"target\", axis=1), data[\"target\"], test_size=0.2, random_state=83)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and Training a Simple Neural Network Model\n",
    "\n",
    "First model we trained was a simple NN model. We used TensorFlow and Keras to define, compile, and train a simple neural network model for the Iris dataset classification task. The model has one hidden layer with eight neurons and an output layer with three neurons (one for each iris species). The activation function used in the hidden layer is *ReLU*, and the output layer uses the *softmax* activation function.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `Sequential`: A linear stack of layers from Keras for building neural network models layer by layer.\n",
    "- `Dense`: A densely-connected neural network layer from Keras.\n",
    "- `to_categorical`: Converts a class vector (integers) to binary class matrix from Keras.\n",
    "- `model.compile`: Configures the model for training with the specified optimizer, loss function, and metrics.\n",
    "- `model.fit`: Trains the model on the training data and evaluates it on the validation data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=X_train.shape[1], activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train_categorical, epochs=300, batch_size=32, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and Training a Deep Neural Network Model\n",
    "\n",
    "Next model we tried to train was a deep neural network model. The deep model has three hidden layers with 54, 18, and 9 neurons, respectively, and an output layer with three neurons (one for each iris species). All hidden layers use the *ReLU* activation function, and the output layer uses the *softmax* activation function. This code cell aims to explore a deeper architecture for the neural network model, potentially capturing more complex patterns in the data compared to the previous model.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `Sequential`, `Dense`: Similar to the previous neural network model definition and training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Deep neural net\n",
    "deep_model = Sequential([\n",
    "    Dense(54, input_dim=X_train.shape[1], activation=\"relu\"),\n",
    "    Dense(18, activation=\"relu\"),\n",
    "    Dense(9, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "deep_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "deep_model.fit(X_train, y_train_categorical, epochs=300, batch_size=32,\n",
    "               validation_data=(X_test, y_test_categorical))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and Training a Convolutional Neural Network (CNN) Model\n",
    "\n",
    "The final model we explored was the Convolutional Neural Network (CNN) model. The CNN model includes a 1D convolutional layer, max pooling layer, flattening layer, one hidden layer with 16 neurons and an output layer with three neurons. It is designed to capture spatial patterns in the input data.\n",
    "\n",
    "We explored the use of a CNN architecture for the Iris dataset classification. CNNs are particularly effective at capturing spatial dependencies in data, and this experiment aims to assess whether this architecture improves model performance.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `Conv1D`, `MaxPooling1D`, `Flatten`: Layers from Keras for constructing a 1D CNN.\n",
    "- `to_categorical`: Converts a class vector (integers) to binary class matrix from Keras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CNN\n",
    "import numpy as np\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# Reshape the data for CNN (assuming X_train and X_test are DataFrames)\n",
    "X_train_reshaped = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1])\n",
    "X_test_reshaped = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "print(\"Reshaped X_train shape:\", X_train_reshaped.shape)\n",
    "print(\"Reshaped X_test shape:\", X_test_reshaped.shape)\n",
    "\n",
    "# Assuming y_train and y_test are one-hot encoded\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Model definition\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation=\"relu\", input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Model compilation\n",
    "model_cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model training\n",
    "model_cnn.fit(X_train_reshaped, y_train_categorical, epochs=300, batch_size=32, validation_data=(X_test_reshaped, y_test_categorical))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the Neural Network Models\n",
    "\n",
    "After the training of the models, we evaluated their performance on the test data. To focus only on different network architectures, we used the same parameters for model creation and training. All models used the *categorical crossentropy* as the loss function and the *Adam* optimizer for training. Epochs for each training were set to 300 and batch size to 32.\n",
    "\n",
    "For evaluation purpose we wrote a function that evaluates each model with the usage of following metrics:\n",
    "- Accuracy of the model on the test data.\n",
    "- Classification report containing precision, recall, f1-score, and support.\n",
    "- Confusion matrix showing the true positive, true negative, false positive, and false negative values.\n",
    "\n",
    "We also wrote a function that renders a provided confusion matrix. We call this function for each model to visualize the confusion matrices.\n",
    "\n",
    "### Functions/Modules Used\n",
    "- `accuracy_score`: Computes the accuracy classification score from scikit-learn.\n",
    "- `classification_report`: Builds a text report showing the main classification metrics from scikit-learn.\n",
    "- `confusion_matrix`: Computes the confusion matrix to evaluate the accuracy of classification from scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy\n",
    "from typing import Tuple, Any, Union\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(eval_model, eval_x_test, eval_y_test, label: str) -> Tuple[float, Union[str, dict], Any]:\n",
    "    print(\"-------------\", label, \"-------------\",\"\\n\")\n",
    "    # Predictions on test data\n",
    "    y_pred = tf.argmax(eval_model.predict(eval_x_test), axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(eval_y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(eval_y_test, y_pred, zero_division=numpy.nan)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(eval_y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    return accuracy, class_report, conf_matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, class_names) -> None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "result_simple_nn = evaluate_model(model, X_test, y_test, \"Simple NN Model\")\n",
    "plot_confusion_matrix(result_simple_nn[2], class_names=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "\n",
    "result_deep_nn = evaluate_model(deep_model, X_test, y_test, \"Deep NN Model\")\n",
    "plot_confusion_matrix(result_deep_nn[2], class_names=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "\n",
    "result_cnn = evaluate_model(model_cnn, X_test, y_test, \"Convolutional NN Model\")\n",
    "plot_confusion_matrix(result_cnn[2], class_names=[\"Class 0\", \"Class 1\", \"Class 2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "In this assignment, we explored the application of various neural network architectures for the classification task on the Iris dataset. The dataset contains samples of three species of iris flowers, and our goal was to classify them accurately using different neural network models (architectures).\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Simple Neural Network Model:**\n",
    "   - A simple neural network with one hidden layer yielded satisfactory results, achieving an accuracy above the 90% threshold.\n",
    "   - The model used the ReLU activation function in the hidden layer and softmax in the output layer.\n",
    "   - Evaluation metrics such as accuracy, precision, recall, and the confusion matrix provided insights into the model's performance.\n",
    "\n",
    "2. **Deep Neural Network Model:**\n",
    "   - Increasing the depth of the neural network to three hidden layers with more neurons did not notably improve classification accuracy.\n",
    "   - Despite having a more complex architecture, the deeper model did not significantly outperform the simpler neural network on the Iris dataset.\n",
    "\n",
    "3. **Convolutional Neural Network (CNN) Model:**\n",
    "   - Introducing a 1D CNN architecture for a dataset like Iris, which lacks spatial dependencies, did not lead to considerable performance improvement.\n",
    "   - Using such a complex network as CNN is rather \"overkill\" for a small, simple and straightforward dataset like Iris\n",
    "   - CNNs are more effective for tasks where spatial patterns and dependencies are crucial, such as image classification.\n",
    "\n",
    "4. **Model Selection:**\n",
    "   - For datasets with simple structures like Iris, a moderately sized simple neural network is often sufficient.\n",
    "   - Deeper architectures and convolutional layers may not necessarily lead to better performance and can increase model complexity without corresponding benefits.\n",
    "\n",
    "### Future Optimizations\n",
    "\n",
    "1. **Additional Datasets:**\n",
    "   - Experimenting with a more diverse set of datasets could provide further insights into the strengths and limitations of different neural network architectures.\n",
    "\n",
    "2. **Hyperparameter Optimization:**\n",
    "   - Further experimentation with hyperparameter tuning, such as adjusting learning rates, batch sizes, and the number of epochs, could potentially enhance model performance.\n",
    "   - Extensive hyperparameter tuning and optimization could be explored to identify the most effective set of parameters for each model.\n",
    "\n",
    "3. **Ensemble Approaches:**\n",
    "   - Combining predictions from multiple models using ensemble techniques might enhance overall classification performance.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, selecting an appropriate neural network architecture is a task-specific decision, and understanding the nature of the data is crucial for achieving optimal results. While simple models may suffice for certain datasets, more complex architectures should be reserved for tasks that demand their specific capabilities.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
